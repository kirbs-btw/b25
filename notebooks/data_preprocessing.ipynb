{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_df = pd.read_csv(\n",
    "    \"../data/csv/playlists_dataset/spotify_dataset.csv\", on_bad_lines=\"skip\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = base_df[[\"trackname\", \"playlistname\", \"artistname\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping rows with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = clean_df.dropna(how=\"any\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the var types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trackname       string[python]\n",
      "playlistname    string[python]\n",
      "artistname      string[python]\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "clean_df[\"trackname\"] = clean_df[\"trackname\"].astype(\"string\")\n",
    "clean_df[\"playlistname\"] = clean_df[\"playlistname\"].astype(\"string\")\n",
    "clean_df[\"artistname\"] = clean_df[\"artistname\"].astype(\"string\")\n",
    "\n",
    "print(clean_df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining trackname and artistname"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining the trackname and artistname into one string identify individual songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df[\"track_and_artist\"] = clean_df[\"trackname\"] + \" \" + clean_df[\"artistname\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering wrong playlists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's pretty sure that the extremly long playlists exist because of equal names of different playlists like \"love\" or \"rock\" ... \n",
    "Need to investigate if every user id is connected to one playlist or how it works out if multiple users work on one playlist... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "playlist_sizes = clean_df.groupby(\"playlistname\").size()\n",
    "\n",
    "upper_5_percent_threshold = playlist_sizes.quantile(0.95)\n",
    "\n",
    "valid_playlists = playlist_sizes[playlist_sizes <= upper_5_percent_threshold].index\n",
    "\n",
    "filtered_df = clean_df[clean_df[\"playlistname\"].isin(valid_playlists)]\n",
    "\n",
    "clean_df = filtered_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df.to_csv(\"../data/csv/playlists_dataset/playlist_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/csv/playlists_dataset/playlist_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_playlist_np = clean_df.groupby(\"playlistname\")[\"track_and_artist\"].apply(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_playlist = tokenized_playlist_np.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extrapolate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 3\n",
    "\n",
    "temp_arr = tokenized_playlist_np\n",
    "\n",
    "extrapolated_data = tokenized_playlist\n",
    "\n",
    "# Shuffle each sub-array n times and collect results\n",
    "for subarray in temp_arr:\n",
    "    for _ in range(n):\n",
    "        extrapolated_data.append(np.random.permutation(subarray).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "629280"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(extrapolated_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training and test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data = extrapolated_data\n",
    "\n",
    "# split value\n",
    "split_fac = 0.9\n",
    "\n",
    "max_idx = len(tokenized_data) - 1\n",
    "anchor = int(max_idx * split_fac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = tokenized_playlist[:anchor]\n",
    "test_set = tokenized_playlist[anchor:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/tokenized_data/playlist_names/dataset_train.pkl\", \"wb\") as f:\n",
    "    pickle.dump(training_set, f)\n",
    "\n",
    "\n",
    "with open(\"../data/tokenized_data/playlist_names/dataset_test.pkl\", \"wb\") as f:\n",
    "    pickle.dump(test_set, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
