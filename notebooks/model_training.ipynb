{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B25 - Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/tokenized_data/playlist_names/dataset_train.pkl', 'rb') as f:\n",
    "    train_dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = Word2Vec(sentences=train_dataset, workers=10, vector_size=50, window=5, min_count=1, sg=0)\n",
    "word2vec_model.save(\"../models/b25-sn-v50/b25-sn-v50.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = Word2Vec(sentences=train_dataset, workers=10, vector_size=256, window=5, min_count=1, sg=0)\n",
    "word2vec_model.save(\"../models/b25-sn-v256/b25-sn-v256-a.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = Word2Vec(sentences=train_dataset, workers=10, vector_size=256, window=10, min_count=1, sg=0)\n",
    "word2vec_model.save(\"../models/b25-sn-v256/b25-sn-v256-b.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = Word2Vec(sentences=train_dataset, workers=10, vector_size=256, window=20, min_count=1, sg=0)\n",
    "word2vec_model.save(\"../models/b25-sn-v256/b25-sn-v256-c.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = Word2Vec(sentences=train_dataset, workers=10, vector_size=256, window=20, min_count=1, sg=1, ns_exponent=0.0)\n",
    "word2vec_model.save(\"../models/b25-sn-v256/b25-sn-v256-d.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = Word2Vec(sentences=train_dataset, workers=10, vector_size=512, window=100, min_count=1, sg=0)\n",
    "word2vec_model.save(\"../models/b25-sn-v512/b25-sn-v512-a.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = Word2Vec(sentences=train_dataset, workers=10, vector_size=512, window=100, min_count=1, sg=1, ns_exponent=0.0)\n",
    "word2vec_model.save(\"../models/b25-sn-v512/b25-sn-v512-b.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from song2vec import Song2Vec\n",
    "\n",
    "model = Song2Vec(train_dataset[0:2000], vector_size=512, epochs=5, learning_rate=0.015)\n",
    "\n",
    "\n",
    "model.save(\"../models/b25-sn-v512/b25-sn-v512-g.model\")\n",
    "print(\"Model saved successfully.\")\n",
    "\n",
    "# need some working with nan values in the dataset... or with nan values in the keys...\n",
    "\n",
    "# 7.803 for 100\n",
    "# 5670.13 * 7.803 that would be 30,7 Days\n",
    "\n",
    "# programming it in c took it down to\n",
    "# 5670.13 * 1.46 that would be 8278,38 min = 5,74 Days \n",
    "\n",
    "# generally it's an issues with floats in there\n",
    "# need to cast every element of the list to string and also somehow ignore the nan values\n",
    "# some data cleaning is needed to be done\n",
    "\n",
    "# so i tried to let it train for 1593min and it did not finish...\n",
    "# i need to optimize the whole thing way more to get some real numbers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from algo_testing import song2vec_a\n",
    "\n",
    "model = song2vec_a.Song2VecA(train_dataset[0:100], vector_size=512, epochs=5, learning_rate=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.nearest(train_dataset[0][0], 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from algo_testing import song2vec_c\n",
    "\n",
    "model = song2vec_c.Song2VecC(train_dataset[0:10000], vector_size=512, epochs=5, learning_rate=0.015)\n",
    "\n",
    "with open(\"../models/b25-sn-v512/b25-sn-v512-l.pkl\", 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Param\n",
    "\n",
    "#### b25-sn-v50\n",
    "> vector_size=50, window=5, min_count=1, sg=0\n",
    "\n",
    "#### b25-sn-v256-a\n",
    "> vector_size=256, window=5, min_count=1, sg=0\n",
    "\n",
    "#### b25-sn-v256-b\n",
    "> vector_size=256, window=10, min_count=1, sg=0\n",
    "\n",
    "#### b25-sn-v256-c\n",
    "> vector_size=256, window=20, min_count=1, sg=0\n",
    "\n",
    "#### b25-sn-v256-d\n",
    "> vector_size=256, window=20, min_count=1, sg=1, ns_exponent=0.0\n",
    "\n",
    "#### b25-sn-v512-a\n",
    "> vector_size=512, window=100, min_count=1, sg=0\n",
    "\n",
    "#### b25-sn-v512-b\n",
    "> vector_size=512, window=100, min_count=1, sg=1, ns_exponent=0.0\n",
    "\n",
    "#### b25-sn-v512-c - CBOS\n",
    "> vector_size=512, epochs=5, learning_rate=0.01\n",
    "\n",
    "#### b25-sn-v512-d - CBOS\n",
    "> vector_size=512, epochs=5, learning_rate=0.015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variation\n",
    "CBOW and other learning algorithms focus more on the words near the target word. That makes sense for Natural Language Understanding, but I'm working with playlists, where the weighting of context words is irrelevant.\n",
    "\n",
    "There are some ways to minimize this, but I might need to implement my own training algorithm to eliminate these types of caveats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = Word2Vec(\n",
    "    sentences=train_dataset, \n",
    "    vector_size=256, \n",
    "    window=20,\n",
    "    min_count=1, \n",
    "    workers=10,\n",
    "    sg=1,            \n",
    "    ns_exponent=0.0\n",
    ")\n",
    "\"\"\"\n",
    "sg=1 means it is using the Skip Gram Algorithm wich is a bit less biased towards\n",
    "closer context words\n",
    "Also ns_exponent=0.0 means equal weighting for negative sampling\n",
    "\"\"\"\n",
    "\n",
    "word2vec_model.save(\"../models/b25-sn-v256/b25-sn-v256-e.model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
