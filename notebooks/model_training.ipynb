{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B25 - Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/tokenized_data/playlist_names/dataset_train.pkl', 'rb') as f:\n",
    "    train_dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = Word2Vec(sentences=train_dataset, workers=10, vector_size=50, window=5, min_count=1, sg=0)\n",
    "word2vec_model.save(\"../models/b25-sn-v50/b25-sn-v50.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = Word2Vec(sentences=train_dataset, workers=10, vector_size=256, window=5, min_count=1, sg=0)\n",
    "word2vec_model.save(\"../models/b25-sn-v256/b25-sn-v256-a.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = Word2Vec(sentences=train_dataset, workers=10, vector_size=256, window=10, min_count=1, sg=0)\n",
    "word2vec_model.save(\"../models/b25-sn-v256/b25-sn-v256-b.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = Word2Vec(sentences=train_dataset, workers=10, vector_size=256, window=20, min_count=1, sg=0)\n",
    "word2vec_model.save(\"../models/b25-sn-v256/b25-sn-v256-c.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = Word2Vec(sentences=train_dataset, workers=10, vector_size=256, window=20, min_count=1, sg=1, ns_exponent=0.0)\n",
    "word2vec_model.save(\"../models/b25-sn-v256/b25-sn-v256-d.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = Word2Vec(sentences=train_dataset, workers=10, vector_size=512, window=100, min_count=1, sg=0)\n",
    "word2vec_model.save(\"../models/b25-sn-v512/b25-sn-v512-a.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = Word2Vec(sentences=train_dataset, workers=10, vector_size=512, window=100, min_count=1, sg=1, ns_exponent=0.0)\n",
    "word2vec_model.save(\"../models/b25-sn-v512/b25-sn-v512-b.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "567013\n",
      "Epoche: 0\n",
      "epochwrapup\n",
      "mainskips 0\n",
      "sub skips 75\n",
      "total sub 1192358\n",
      "Epoche: 1\n",
      "epochwrapup\n",
      "mainskips 0\n",
      "sub skips 75\n",
      "total sub 1192358\n",
      "Epoche: 2\n",
      "epochwrapup\n",
      "mainskips 0\n",
      "sub skips 75\n",
      "total sub 1192358\n",
      "Epoche: 3\n",
      "epochwrapup\n",
      "mainskips 0\n",
      "sub skips 75\n",
      "total sub 1192358\n",
      "Epoche: 4\n",
      "epochwrapup\n",
      "mainskips 0\n",
      "sub skips 75\n",
      "total sub 1192358\n",
      "7.8039870262146\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "root_dir = os.path.abspath(\"../song2vec\")  \n",
    "sys.path.append(root_dir)\n",
    "\n",
    "import song2vec.submodule\n",
    "\n",
    "print(len(train_dataset))\n",
    "\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "a = song2vec.submodule.Song2Vec(train_dataset[0:100], vector_size=512, epochs=5, learning_rate=0.0005)\n",
    "end = time.time()\n",
    "\n",
    "print(end-start)\n",
    "\n",
    "a.save(\"../models/b25-sn-v512/b25-sn-v512-c.pkl\")\n",
    "\n",
    "# need some working with nan values in the dataset... or with nan values in the keys...\n",
    "\n",
    "# 7.803 for 100\n",
    "# 5670.13 * 7.803 that would be 1.4 years of training..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Param\n",
    "\n",
    "#### b25-sn-v50\n",
    "> vector_size=50, window=5, min_count=1, sg=0\n",
    "\n",
    "#### b25-sn-v256-a\n",
    "> vector_size=256, window=5, min_count=1, sg=0\n",
    "\n",
    "#### b25-sn-v256-b\n",
    "> vector_size=256, window=10, min_count=1, sg=0\n",
    "\n",
    "#### b25-sn-v256-c\n",
    "> vector_size=256, window=20, min_count=1, sg=0\n",
    "\n",
    "#### b25-sn-v256-d\n",
    "> vector_size=256, window=20, min_count=1, sg=1, ns_exponent=0.0\n",
    "\n",
    "#### b25-sn-v512-a\n",
    "> vector_size=512, window=100, min_count=1, sg=0\n",
    "\n",
    "#### b25-sn-v512-b\n",
    "> vector_size=512, window=100, min_count=1, sg=1, ns_exponent=0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variation\n",
    "CBOW and other learning algorithms focus more on the words near the target word. That makes sense for Natural Language Understanding, but I'm working with playlists, where the weighting of context words is irrelevant.\n",
    "\n",
    "There are some ways to minimize this, but I might need to implement my own training algorithm to eliminate these types of caveats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = Word2Vec(\n",
    "    sentences=train_dataset, \n",
    "    vector_size=256, \n",
    "    window=20,\n",
    "    min_count=1, \n",
    "    workers=10,\n",
    "    sg=1,            \n",
    "    ns_exponent=0.0\n",
    ")\n",
    "\"\"\"\n",
    "sg=1 means it is using the Skip Gram Algorithm wich is a bit less biased towards\n",
    "closer context words\n",
    "Also ns_exponent=0.0 means equal weighting for negative sampling\n",
    "\"\"\"\n",
    "\n",
    "word2vec_model.save(\"../models/b25-sn-v256/b25-sn-v256-e.model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
